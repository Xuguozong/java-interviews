## 进程和线程上下文切换的区别

*进程上下文切换开销*

> 直接开销

```
1. 切换页表全局目录
2. 切换内核态堆栈
3. 切换硬件上下文（进程恢复前，必须装入寄存器的数据统称为硬件上下文）
	3.1 ip（instruction pointer）:指向当前执行指令的下一条
	3.2 bp（base pointer）：用于存放执行中的函数对应的栈帧的栈底地址
	3.3 sp(stack pointer): 用于存放执行中的函数对应的栈帧的栈顶地址
	3.4 cr3:页目录基址寄存器，保存页目录表的物理地址
	......
4. 刷新TLB -> 页表缓冲（processor’s Translation Lookaside Buffer (TLB)）
5. 系统调度器的代码执行

总体来说，进程切换分两步：
1.切换页目录以使用新的地址空间
2.切换内核栈和硬件上下文。
```

> 间接开销

```
扰乱处理器的缓存机制，已缓存的热数据作废
```

**线程的上下文切换是不需要切换新的地址空间的**



## 上下文切换的监控

*上下文切换比较的消耗CPU时间*

```
# sudo vmstat 1  
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----  
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st  
 2  0      0 595504   5724 190884    0    0   295   297    0    0 14  6 75  0  4  
 5  0      0 593016   5732 193288    0    0     0    92 19889 29104 20  6 67  0  7  
 3  0      0 591292   5732 195476    0    0     0     0 20151 28487 20  6 66  0  8  
 4  0      0 589296   5732 196800    0    0   116   384 19326 27693 20  7 67  0  7  
 4  0      0 586956   5740 199496    0    0   216    24 18321 24018 22  8 62  0  8
 
// cs 列表示1s内系统发生的上下文切换次数
```

```
# sudo sar -w 1  
proc/s  
     Total number of tasks created per second.  
cswch/s  
     Total number of context switches per second.
   
11:19:20 AM    proc/s   cswch/s  
11:19:21 AM    110.28  23468.22  
11:19:22 AM    128.85  33910.58  
11:19:23 AM     47.52  40733.66  
11:19:24 AM     35.85  30972.64  
11:19:25 AM     47.62  24951.43  
11:19:26 AM     47.52  42950.50  
......
```

```
# sudo pidstat -w 1  
11:07:56 AM       PID   cswch/s nvcswch/s  Command
11:07:56 AM     32316      4.00      0.00  php-fpm  
11:07:56 AM     32508    160.00     34.00  php-fpm  
11:07:56 AM     32726    131.00      8.00  php-fpm  
# sudo pidstat -w -p 48863  // 根据 PID 监控
// sudo yum/apt install sysstat
// cswch/s 每秒主动切换上下文的次数(如阻塞等待时)
// nvcswch/s 每秒被动上下文切换次数(如CPU时间片用完)
```

```
## 查看具体某个进程的上下文切换总情况，可以在/proc接口下直接看，不过这个是总值
# sudo grep ctxt /proc/32583/status  
voluntary_ctxt_switches:        573066  
nonvoluntary_ctxt_switches:     89260
```

### 不错的观点

```
当可运行的 thread 越多的时候，上下文切换开销基本不变，但是，调度开销上去了。
具体点，增加的时间就是系统找到下一个可运行的线程的时间。
听起来， O(log(n)) 的开销不大，但是一个系统内的活跃线程数量并不会完全受我们控制，
尤其当你的中间件里存在着大量出IO操作的时候(也就是通常说的高并发的情况下)。
log(n) 的复杂度会导致线程的响应时间会增加的更多，这样，
大量的线程得不到响应，反而会使系统内可运行的线程数量又变多。
这也是所谓线程模型处理高并发的短处，甚至，有好事儿者（挑战者？）还专门发明了一个 C10K 问题。
这里不得不聊一下 Go 语言，我们知道，Go 语言默认会开启一个最大同时运行的并发数，
一般等价于系统核心数（超线程）。但是其后台实际的线程数有多少呢，
其实我们也无法控制。如果你的系统里存在大量非内置的 syscall（比如上面的getpid） 或者存在大量的 cgo 调用，
你很可能的到一个跑着几百几千个线程的怪物程序。


正确的做法

解决 C10K 问题，正确的入手方式是改掉传统的线程模型，也就是着手减少系统 runnable 线程数量。
为了这一目标，linux 引入了多路复用技术。
我们来假设一下，如果一个线程调用了一次 I/O 操作而对应的这个 fd 并没有数据给他或者返回，
那么这个线程会立马问自己三个问题：我是谁，我还可以动么，我可以阻塞么。
三个问题直击内核心灵，内核大手一挥，你阻塞吧。
可想而知，这个样的代价是巨大的。一次阻塞态切换，不仅包含这上下文切换的开销，
还有可能带来 syscall resume 之后的大量的调度开销。
因此，我们诞生了一个线程用多个fd的技术：select/poll。
但是仔细看看，select 和 poll 的模型还是挺傻的，因为你不知道具体是谁发生了 I/O 就绪事件,
我只能非常莽的轮询所有 fd 。这显然非常低效。
epoll 改进了上述模型，用两个链表维护了 IO 事件就绪的事件列表，用户在得到 epoll 事件之后，
拿到的一定是所有的发生用户期待的 I/O 事件的列表。这样无疑快了很多。
所以 epoll 在本质上，其实是一种简单的就绪通知机制。
```

